{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzdjiC0pCUqC",
        "outputId": "8c8b1df4-b672-4743-9064-0b506442c06b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.55)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.3.31)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.55)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.24)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.63)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (0.3.31)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (2.11.3)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tavily (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tavily\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.55)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.55)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.31)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.19.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain_community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain_community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain_community) (2.33.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.3.31)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.55)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.24)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.63)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (0.3.31)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (2.11.3)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Requirement already satisfied: langfuse in /usr/local/lib/python3.11/dist-packages (2.60.3)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from langfuse) (4.9.0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from langfuse) (2.2.1)\n",
            "Requirement already satisfied: httpx<1.0,>=0.15.4 in /usr/local/lib/python3.11/dist-packages (from langfuse) (0.28.1)\n",
            "Requirement already satisfied: idna<4.0,>=3.7 in /usr/local/lib/python3.11/dist-packages (from langfuse) (3.10)\n",
            "Requirement already satisfied: packaging<25.0,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langfuse) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0,>=1.10.7 in /usr/local/lib/python3.11/dist-packages (from langfuse) (2.11.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langfuse) (2.32.3)\n",
            "Requirement already satisfied: wrapt<2.0,>=1.14 in /usr/local/lib/python3.11/dist-packages (from langfuse) (1.17.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.4.0->langfuse) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.4.0->langfuse) (4.13.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.15.4->langfuse) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.15.4->langfuse) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=1.10.7->langfuse) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=1.10.7->langfuse) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=1.10.7->langfuse) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langfuse) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langfuse) (2.3.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.3.31)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.14)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.55)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.24)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.63)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.75.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.19.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (4.13.2)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.3.31)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.55)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.24)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.63)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (0.3.31)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (2.11.3)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install langgraph\n",
        "!pip install tavily\n",
        "!pip install transformers\n",
        "!pip install openai\n",
        "!pip install beautifulsoup4\n",
        "!pip install requests\n",
        "!pip install tiktoken\n",
        "!pip install --upgrade langchain\n",
        "!pip install langchain_community\n",
        "!pip install --upgrade langgraph\n",
        "!pip install -q langchain langchain-community langgraph\n",
        "!pip install -q langchain-google-genai\n",
        "!pip install -q tavily-python\n",
        "!pip install -q beautifulsoup4 requests\n",
        "!pip install -q pandas\n",
        "!pip install langfuse\n",
        "!pip install langchain langgraph langchain_openai langchain_community\n",
        "!pip install -qU langchain-openai\n",
        "!pip install --upgrade langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata #Access Secret key\n",
        "\n",
        "# Load Google API Key\n",
        "try:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "    print(\"Google API Key loaded.\")\n",
        "except:\n",
        "    print(\"Google API Key not found in Colab secrets. Please add it.\")\n",
        "\n",
        "# Load Tavily API Key\n",
        "try:\n",
        "    os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")\n",
        "    print(\"Tavily API Key loaded.\")\n",
        "except:\n",
        "    print(\"Tavily API Key not found in Colab secrets. Please add it.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjQ6cXW-A63-",
        "outputId": "8832fc08-b36d-4c90-cdeb-17b9d687a6ef"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google API Key loaded.\n",
            "Tavily API Key loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Using Google Gemini\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# Import tool integrations\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "print(\"Libraries imported successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsgFfzG6-Deu",
        "outputId": "c5a7c16a-e1bb-469b-ca78-0bdaf0fda4d6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai # For direct Gemini testing\n",
        "from tavily import TavilyClient # For direct Tavily testing\n",
        "from google.colab import userdata # Colab specific for accessing secrets if not using os.environ directly after userdata.get\n",
        "\n",
        "print(\"--- Starting API Key Test ---\")\n",
        "\n",
        "# --- Test Google Gemini API Key ---\n",
        "print(\"\\nTesting Google Gemini API...\")\n",
        "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "if not google_api_key:\n",
        "    print(\" Google_API_KEY not found in environment variables. Skipping Gemini test.\")\n",
        "else:\n",
        "    try:\n",
        "        # Configure the client with the API key\n",
        "        genai.configure(api_key=google_api_key)\n",
        "        genai.list_models()\n",
        "\n",
        "        print(\" Google Gemini API Key is valid and working.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Google Gemini API Key test failed: {e}\")\n",
        "        print(\"   Possible issues: Invalid key, network problems, or service outage.\")\n",
        "\n",
        "\n",
        "# --- Test Tavily API Key ---\n",
        "print(\"\\nTesting Tavily API...\")\n",
        "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
        "\n",
        "if not tavily_api_key:\n",
        "    print(\" TAVILY_API_KEY not found in environment variables. Skipping Tavily test.\")\n",
        "else:\n",
        "    try:\n",
        "        # Instantiate the Tavily client\n",
        "        tavily_client = TavilyClient(api_key=tavily_api_key)\n",
        "        response = tavily_client.search(query=\"test query\", max_results=1)\n",
        "\n",
        "        if response and 'results' in response:\n",
        "             print(\" Tavily API Key is valid and working.\")\n",
        "        else:\n",
        "             # This case might happen if the key is valid but the search failed for another reason\n",
        "             print(f\" Tavily API Key test failed: Search returned unexpected response. {response}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Tavily API Key test failed: {e}\")\n",
        "        print(\"   Possible issues: Invalid key, network problems, or service outage.\")\n",
        "\n",
        "print(\"\\n--- API Key Test Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zox3KAxq-bB3",
        "outputId": "db1741e5-fe59-4ff5-d224-b6eb8f241032"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting API Key Test ---\n",
            "\n",
            "Testing Google Gemini API...\n",
            " Google Gemini API Key is valid and working.\n",
            "\n",
            "Testing Tavily API...\n",
            " Tavily API Key is valid and working.\n",
            "\n",
            "--- API Key Test Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any, TypedDict\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "# --- 1. Defining the State of Graph ---\n",
        "class ResearchState(TypedDict):\n",
        "    \"\"\"Represents the state of the research and drafting process.\"\"\"\n",
        "    user_query: str\n",
        "    research_plan: List[str]\n",
        "    search_queries: List[str]\n",
        "    raw_search_results: List[Dict[str, Any]]\n",
        "    crawled_content: List[Dict[str, str]]\n",
        "    processed_information: str\n",
        "    draft_answer: str\n",
        "    review_feedback: str\n",
        "    review_decision: str"
      ],
      "metadata": {
        "id": "rJM-ljipvzVH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, TypedDict\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from google.colab import userdata\n",
        "from langgraph.graph import StateGraph, END\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time # For adding delays in crawling\n",
        "\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0)\n",
        "tavily_tool = TavilySearchResults(max_results=5)\n",
        "\n",
        "def escape_curly_braces(text: Any) -> str:\n",
        "    \"\"\"Escapes single curly braces in text for f-string safety.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    # Replace } with }} and { with {{\n",
        "    return text.replace('}', '}}').replace('{', '{{')\n",
        "\n",
        "# --- 2. Implementing Each Agent as a Node Function ---\n",
        "# --- 2a. Query Analyst Agent  ---\n",
        "def query_analyst_node(state: ResearchState) -> Dict[str, Any]:\n",
        "    \"\"\"Analyzes the user query and creates a research plan (list of questions).\"\"\"\n",
        "    print(\"\\n---Executing Query Analyst---\")\n",
        "    user_query = state['user_query']\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a Query Analyst. Your role is to deeply analyze user queries.\n",
        "        First, conduct a 5W1H analysis (Who, What, When, Where, Why, and How) of the user's query to fully understand its scope and requirements.\n",
        "        Based on this 5W1H analysis, break down the complex user query into a list of specific, actionable research questions.\n",
        "        Focus on identifying the key pieces of information needed to fully answer the user's request, informed by your 5W1H analysis.\n",
        "        Respond with ONLY a numbered list of research questions. Do not include the 5W1H analysis itself, or any other text (like introductory/concluding remarks) before or after the list.\"\"\"),\n",
        "        (\"user\", f\"Analyze the following query and list the research questions, based on your 5W1H analysis: {user_query}\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    research_plan_str = chain.invoke({})\n",
        "    research_plan = [q.strip() for q in research_plan_str.split('\\n') if q.strip() and any(char.isdigit() for char in q.split('.')[0])] # Basic check for numbering\n",
        "\n",
        "    print(f\"Generated Research Plan: {research_plan}\")\n",
        "    return {\"research_plan\": research_plan}\n",
        "\n",
        "\n",
        "# --- 2b. Research Coordinator Node (Implementation Provided Previously) ---\n",
        "def research_coordinator_node(state: ResearchState) -> Dict[str, Any]:\n",
        "    \"\"\"Takes the research plan and generates specific search queries.\"\"\"\n",
        "    print(\"\\n---Executing Research Coordinator---\")\n",
        "    research_plan = state.get('research_plan', []) # Get plan safely\n",
        "\n",
        "    if not research_plan:\n",
        "        print(\"No research plan provided by Query Analyst. Skipping search query generation.\")\n",
        "        return {\"search_queries\": []}\n",
        "\n",
        "    plan_text = \"\\n\".join([f\"- {q}\" for q in research_plan]) # Format for prompt\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a Research Coordinator. You receive a list of research questions or tasks.\n",
        "        Your role is to convert these into concise and effective search engine query strings.\n",
        "        Aim for diversity if needed to cover different angles.\n",
        "        Respond with ONLY a list of search query strings, one per line. Do not include numbering or bullet points or any other text.\"\"\"), # Simplified output parsing\n",
        "        (\"user\", f\"Convert these research questions/tasks into search queries:\\n{plan_text}\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    search_queries_str = chain.invoke({})\n",
        "    search_queries = [q.strip() for q in search_queries_str.split('\\n') if q.strip()]\n",
        "\n",
        "    print(f\"Generated Search Queries: {search_queries}\")\n",
        "    return {\"search_queries\": search_queries}\n",
        "\n",
        "\n",
        "# --- 2c. Researcher Agent (Your Code - Updated to read search_queries) ---\n",
        "\n",
        "def researcher_node(state: ResearchState) -> Dict[str, Any]:\n",
        "    \"\"\"Executes search queries based on the search_queries state and collects raw results.\"\"\"\n",
        "    print(\"\\n---Executing Researcher---\")\n",
        "    search_queries = state.get('search_queries', [])\n",
        "\n",
        "    if not search_queries:\n",
        "        print(\"No search queries provided by coordinator. Skipping research.\")\n",
        "        return {\"raw_search_results\": []}\n",
        "\n",
        "    raw_results = []\n",
        "    print(f\"Attempting searches for {len(search_queries)} queries.\")\n",
        "\n",
        "    for query in search_queries:\n",
        "        try:\n",
        "            results = tavily_tool.invoke({\"query\": query})\n",
        "            print(f\"--- Results for query: '{query}' ---\")\n",
        "            if results:\n",
        "                for i, res in enumerate(results):\n",
        "                    url = res.get('url', 'N/A')\n",
        "                    title = res.get('title', 'No Title') # Also print title for context\n",
        "                    print(f\"  {i+1}. [{title}] {url}\")\n",
        "            else:\n",
        "                print(\"  No results found.\")\n",
        "            raw_results.extend(results)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during search for '{query}': {e}\")\n",
        "            pass\n",
        "\n",
        "    print(f\"\\nCollected {len(raw_results)} total raw search results across all queries.\")\n",
        "    return {\"raw_search_results\": raw_results}\n",
        "\n",
        "\n",
        "# --- 2d. Crawler Node (New Implementation) ---\n",
        "def crawler_node(state: ResearchState) -> Dict[str, Any]:\n",
        "    \"\"\"Fetches content from URLs found in raw_search_results.\"\"\"\n",
        "    print(\"\\n---Executing Crawler---\")\n",
        "    raw_results = state.get('raw_search_results', [])\n",
        "\n",
        "    if not raw_results:\n",
        "        print(\"No raw search results with URLs to crawl. Skipping crawl.\")\n",
        "        return {\"crawled_content\": []}\n",
        "    urls = set()\n",
        "    for res in raw_results:\n",
        "        url = res.get('url')\n",
        "        # Basic validation and limit to http/https\n",
        "        if url and isinstance(url, str) and url.startswith('http'):\n",
        "            urls.add(url)\n",
        "\n",
        "    # Convert set back to list\n",
        "    urls = list(urls)\n",
        "\n",
        "    # Limit the number of pages to crawl to avoid excessive requests\n",
        "    # Adjust this limit based on your needs and politeness\n",
        "    max_pages_to_crawl = 5\n",
        "    urls_to_crawl = urls[:max_pages_to_crawl]\n",
        "\n",
        "    print(f\"Found {len(urls)} unique URLs from search. Crawling up to {max_pages_to_crawl}...\")\n",
        "\n",
        "    crawled_content_list = []\n",
        "    # Add a User-Agent header to make requests look more like a browser\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    for i, url in enumerate(urls_to_crawl):\n",
        "        print(f\"Crawling: {url}\")\n",
        "        try:\n",
        "            # Add a small delay to be polite and avoid being blocked\n",
        "            time.sleep(1) # Adjust delay as needed\n",
        "\n",
        "            response = requests.get(url, headers=headers, timeout=10) # Add timeout\n",
        "            response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "            # Use BeautifulSoup to parse the HTML and extract text\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = soup.get_text()\n",
        "\n",
        "            # Basic text cleaning: remove excessive whitespace and newlines\n",
        "            # This helps reduce token count for the LLM\n",
        "            cleaned_text = ' '.join(text.split())\n",
        "\n",
        "            crawled_content_list.append({\"url\": url, \"text\": cleaned_text})\n",
        "            print(f\"Successfully crawled and extracted text from {url}\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {url}: {e}\")\n",
        "\n",
        "\n",
        "    print(f\"Finished crawling. Collected content from {len(crawled_content_list)} pages.\")\n",
        "    # Update the state with the fetched content\n",
        "    return {\"crawled_content\": crawled_content_list}\n",
        "\n",
        "# --- 2e. Information Processor / Synthesizer Agent (Modified) ---\n",
        "def information_processor_node(state: ResearchState) -> Dict[str, Any]:\n",
        "    \"\"\"Synthesizes raw search results and crawled content into structured information.\"\"\"\n",
        "    print(\"\\n---Executing Information Processor---\")\n",
        "    raw_results = state.get('raw_search_results', [])\n",
        "    crawled_content = state.get('crawled_content', [])\n",
        "\n",
        "    if not raw_results and not crawled_content:\n",
        "        print(\"No raw results or crawled content to process.\")\n",
        "        return {\"processed_information\": \"No information found.\"}\n",
        "\n",
        "    # Format and ESCAPE both types of results for the LLM\n",
        "    formatted_search_results = \"\\n---\\n\".join([\n",
        "        f\"Search Snippet - Source: {escape_curly_braces(res.get('url', 'N/A'))}\\nContent: {escape_curly_braces(res.get('content', 'N/A'))}\"\n",
        "        for res in raw_results\n",
        "    ])\n",
        "\n",
        "    formatted_crawled_content = \"\\n---\\n\".join([\n",
        "        f\"Full Page Content - Source: {escape_curly_braces(page.get('url', 'N/A'))}\\nContent: {escape_curly_braces(page.get('text', 'No text extracted.')[:2000])}...\" # Escape and limit length\n",
        "        for page in crawled_content\n",
        "    ])\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are an Information Processor. Synthesize information from both the provided search snippets and the full page content fetched from relevant pages.\n",
        "        Use the search snippets for context and diverse sources, and the full page content for detailed information.\n",
        "        Extract key facts and answer the research questions provided earlier (if available in the state['research_plan']).\n",
        "        Structure the information logically. If conflicting information is found between sources, note it.\n",
        "        Present the synthesized information clearly and concisely.\"\"\"),\n",
        "        (\"user\", f\"\"\"Raw Search Snippets:\n",
        "{formatted_search_results}\n",
        "\n",
        "---\n",
        "\n",
        "Full Page Content:\n",
        "{formatted_crawled_content}\"\"\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    processed_info = chain.invoke({})\n",
        "\n",
        "    print(\"Information processing complete.\")\n",
        "    return {\"processed_information\": processed_info}\n",
        "\n",
        "\n",
        "# --- 2e. Drafting Agent (Your Code) ---\n",
        "\n",
        "def drafting_node(state: ResearchState) -> Dict[str, Any]:\n",
        "    \"\"\"Drafts the final answer based on the processed information and original query.\"\"\"\n",
        "    print(\"\\n---Executing Drafting Agent---\")\n",
        "    user_query = state.get('user_query', 'N/A - Original query missing.')\n",
        "    processed_info = state.get('processed_information', '')\n",
        "\n",
        "    escaped_user_query = escape_curly_braces(user_query)\n",
        "    escaped_processed_info = escape_curly_braces(processed_info)\n",
        "\n",
        "    if not processed_info or processed_info == \"No information found.\":\n",
        "        draft = \"Could not gather sufficient information to answer your query.\"\n",
        "    else:\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", f\"\"\"You are a Drafting Agent. Write a comprehensive answer to the user's original query based ONLY on the processed information provided.\n",
        "            Ensure the answer is clear, well-structured, and directly addresses the user's request.\n",
        "            User Query: {escaped_user_query}\"\"\"), # <-- Use escaped variable\n",
        "            (\"user\", f\"Processed Information:\\n{escaped_processed_info}\") # <-- Use escaped variable\n",
        "        ])\n",
        "\n",
        "        chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "        draft = chain.invoke({})\n",
        "    print(\"Drafting complete.\")\n",
        "    return {\"draft_answer\": draft}\n",
        "\n",
        "\n",
        "# --- 2f. Reviewer Node (Implementation Provided Previously) ---\n",
        "def reviewer_node(state: ResearchState) -> Dict[str, Any]:\n",
        "    \"\"\"Reviews the draft answer, provides feedback, and decides if revision is needed.\"\"\"\n",
        "    print(\"\\n---Executing Reviewer---\")\n",
        "    user_query = state.get('user_query', 'N/A - Original query missing.')\n",
        "    draft_answer = state.get('draft_answer', '')\n",
        "    processed_info = state.get('processed_information', 'N/A - Could not retrieve processed info.') # Use processed info for accuracy check\n",
        "\n",
        "    if not draft_answer or draft_answer == \"Could not gather sufficient information to answer your query.\":\n",
        "        print(\"No valid draft to review. Ending process.\")\n",
        "        return {\"review_feedback\": \"Review skipped: No valid draft generated.\", \"review_decision\": \"END\"}\n",
        "    escaped_user_query = escape_curly_braces(user_query)\n",
        "    escaped_draft_answer = escape_curly_braces(draft_answer)\n",
        "    escaped_processed_info = escape_curly_braces(processed_info)\n",
        "\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", f\"\"\"You are a Reviewer Agent. Your task is to review a draft answer based on the user's original query and the processed information used to create the draft.\n",
        "        Provide constructive feedback on clarity, accuracy, completeness, grammar, and style.\n",
        "        Based on your review, decide if the draft requires significant revisions or is acceptable as a final answer.\n",
        "        Your final decision determines the next step in the workflow.\n",
        "\n",
        "        **IMPORTANT:** Your response MUST start with one of the following exact phrases on the first line, followed by your feedback:\n",
        "        DECISION: REVISE - The draft needs significant changes.\n",
        "        DECISION: ACCEPT - The draft is acceptable.\n",
        "\n",
        "        ---\n",
        "        Original Query: {escaped_user_query} # <-- Use escaped variable\n",
        "        Processed Information (Reference):\n",
        "        {escaped_processed_info} # <-- Use escaped variable\n",
        "        ---\"\"\"),\n",
        "        (\"user\", f\"Draft Answer to Review:\\n{escaped_draft_answer}\\n---\") # <-- Use escaped variable\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    review_output = chain.invoke({})\n",
        "\n",
        "    print(f\"Raw Reviewer Output:\\n{review_output}\")\n",
        "\n",
        "    # ... (Parsing logic remains the same) ...\n",
        "    decision = \"END\" # Default decision if parsing fails\n",
        "    feedback = review_output # Default feedback is the whole output\n",
        "\n",
        "    output_lines = review_output.strip().split('\\n', 1)\n",
        "    if output_lines and output_lines[0].startswith(\"DECISION:\"):\n",
        "        decision_part = output_lines[0].replace(\"DECISION:\", \"\").strip().upper()\n",
        "        if \"REVISE\" in decision_part:\n",
        "            decision = \"REVISE\"\n",
        "        feedback = output_lines[1].strip() if len(output_lines) > 1 else \"No specific feedback provided.\"\n",
        "    else:\n",
        "        feedback = f\"Reviewer did not follow format. Full output: {review_output}\"\n",
        "        decision = \"END\"\n",
        "\n",
        "    print(f\"Parsed Review Decision: {decision}\")\n",
        "    print(f\"Parsed Review Feedback: {feedback}\")\n",
        "\n",
        "    return {\"review_feedback\": feedback, \"review_decision\": decision}\n",
        "\n",
        "\n",
        "def route_review(state: ResearchState) -> str:\n",
        "    \"\"\"\n",
        "    Router function for the reviewer transition.\n",
        "    Determines the next step based on the review_decision in the state.\n",
        "    \"\"\"\n",
        "    print(\"\\n---Executing Review Router (Function)---\")\n",
        "    # Read the decision made by the reviewer node from the state\n",
        "    decision = state.get('review_decision', 'END')\n",
        "    print(f\"Reviewer decision received: {decision}\")\n",
        "\n",
        "    # Based on the decision, return the name of the next node or END string\n",
        "    # These return values MUST match the keys in the mapping below (\"REVISE\", \"END\")\n",
        "    if decision == \"REVISE\":\n",
        "        print(\"Routing back to Drafting Agent for revision.\")\n",
        "        return \"REVISE\" # Return the string key for the mapping\n",
        "    else:\n",
        "        print(\"Review accepted or decision unclear. Ending workflow.\")\n",
        "        return \"END\" # Return the string key for the mapping\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ypDWupCB15gJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langgraph langchain-community langchain-google-genai tavily-python requests beautifulsoup4"
      ],
      "metadata": {
        "id": "rwCI9fLO24y3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Define the Workflow Graph (LangGraph) ---\n",
        "\n",
        "# Define the graph\n",
        "workflow = StateGraph(ResearchState)\n",
        "\n",
        "# Add all the *agent* nodes (The actual steps that perform tasks)\n",
        "# DO NOT add the router function as a node here\n",
        "workflow.add_node(\"query_analyst\", query_analyst_node)\n",
        "workflow.add_node(\"research_coordinator\", research_coordinator_node)\n",
        "workflow.add_node(\"researcher\", researcher_node)\n",
        "workflow.add_node(\"crawler\", crawler_node) # Add the crawler node\n",
        "workflow.add_node(\"information_processor\", information_processor_node)\n",
        "workflow.add_node(\"drafting_agent\", drafting_node)\n",
        "workflow.add_node(\"reviewer\", reviewer_node)\n",
        "\n",
        "\n",
        "# Set the entry point (where the graph starts)\n",
        "workflow.set_entry_point(\"query_analyst\")\n",
        "\n",
        "# Define sequential edges\n",
        "workflow.add_edge(\"query_analyst\", \"research_coordinator\")\n",
        "workflow.add_edge(\"research_coordinator\", \"researcher\")\n",
        "workflow.add_edge(\"researcher\", \"crawler\") # After research (search), go to the crawler\n",
        "workflow.add_edge(\"crawler\", \"information_processor\") # After crawling, go to the information processor\n",
        "\n",
        "# Continue the flow after information processing\n",
        "workflow.add_edge(\"information_processor\", \"drafting_agent\")\n",
        "workflow.add_edge(\"drafting_agent\", \"reviewer\")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"reviewer\",\n",
        "    route_review,\n",
        "    {\n",
        "        \"REVISE\": \"drafting_agent\",\n",
        "        \"END\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# --- 4. Compile the graph ---\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"\\nLangGraph workflow compiled with all agents and conditional routing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzTk5_214zlK",
        "outputId": "04b7ccf1-0f8d-44ef-f021-d304f4c27665"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LangGraph workflow compiled with all agents and conditional routing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Run the Graph (Example Usage) ---\n",
        "\n",
        "# Example usage\n",
        "user_query = \"Explain the concept of Deep Learning and advancment in it\"\n",
        "\n",
        "# Initial state contains just the user query and empty values for everything else\n",
        "initial_state = {\n",
        "    \"user_query\": user_query,\n",
        "    \"research_plan\": [],\n",
        "    \"search_queries\": [],\n",
        "    \"raw_search_results\": [],\n",
        "    \"crawled_content\": [],\n",
        "    \"processed_information\": \"\",\n",
        "    \"draft_answer\": \"\",\n",
        "    \"review_feedback\": \"\",\n",
        "    \"review_decision\": \"\"\n",
        "}\n",
        "\n",
        "\n",
        "print(f\"\\nStarting workflow for query: {user_query}\")\n",
        "\n",
        "\n",
        "# Or use invoke() to just get the final state\n",
        "final_state = app.invoke(initial_state)\n",
        "\n",
        "\n",
        "# --- Access the final output ---\n",
        "print(\"\\n--- Workflow Finished ---\")\n",
        "\n",
        "print(\"\\n--- Final Answer Draft ---\")\n",
        "print(final_state.get('draft_answer', 'No draft generated.'))\n",
        "\n",
        "print(\"\\n--- Final Review Feedback ---\")\n",
        "print(final_state.get('review_feedback', 'No review feedback.'))\n",
        "\n",
        "print(\"\\n--- Final Review Decision ---\")\n",
        "print(final_state.get('review_decision', 'N/A'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHVxJdlhmQ-N",
        "outputId": "6fb280a2-9aea-4704-cf8d-ff80dc14fb88"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting workflow for query: Explain the concept of Deep Learning and advancment in it\n",
            "\n",
            "---Executing Query Analyst---\n",
            "Generated Research Plan: ['1. What is the fundamental definition of Deep Learning?', '2. What are the core components and architecture of a Deep Learning model (e.g., layers, neurons, activation functions)?', '3. What are the different types of Deep Learning architectures (e.g., Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Adversarial Networks (GANs)) and their respective applications?', '4. What are the key advancements in Deep Learning in the last 5-10 years?', '5. What are the major breakthroughs and milestones achieved in Deep Learning research?', '6. How has the computational power and availability of large datasets influenced the advancements in Deep Learning?', '7. What are the current limitations and challenges in Deep Learning?', '8. What are the ethical considerations surrounding the advancements and applications of Deep Learning?', '9. What are the future trends and potential directions of research in Deep Learning?', '10. What are some prominent examples of successful applications of Deep Learning across various domains (e.g., image recognition, natural language processing, healthcare)?', '11. What are the key mathematical and statistical concepts underlying Deep Learning algorithms?', '12. How does Deep Learning compare to other machine learning techniques?', '13. What are the different software libraries and tools used for implementing Deep Learning models?', '14. What are the educational resources available for learning about Deep Learning?', '15. Who are the key researchers and contributors to the field of Deep Learning?']\n",
            "\n",
            "---Executing Research Coordinator---\n",
            "Generated Search Queries: ['\"definition deep learning\"', '\"deep learning architecture components\"', '\"types deep learning architectures applications CNN RNN GAN\"', '\"deep learning advancements last 10 years\"', '\"deep learning breakthroughs milestones\"', '\"computational power datasets impact deep learning\"', '\"deep learning limitations challenges\"', '\"ethical considerations deep learning\"', '\"future trends deep learning research\"', '\"deep learning applications examples image recognition NLP healthcare\"', '\"deep learning mathematical statistical concepts\"', '\"deep learning vs machine learning\"', '\"deep learning software libraries tools TensorFlow PyTorch\"', '\"deep learning online courses tutorials\"', '\"key researchers deep learning\"']\n",
            "\n",
            "---Executing Researcher---\n",
            "Attempting searches for 15 queries.\n",
            "--- Results for query: '\"definition deep learning\"' ---\n",
            "  1. [What is Deep Learning in Marketing? - Adobe Experience Cloud] https://business.adobe.com/blog/basics/deep-learning\n",
            "  2. [What is deep learning? - The Learning Code] https://thelearningcode.school.blog/2021/01/24/what-is-deep-learning/\n",
            "  3. [Definition: deep learning - ComputerLanguage.com] https://www.computerlanguage.com/results.php?definition=deep+learning\n",
            "  4. [Demystifying AI, ML, Deep Learning, and Data Science - Medium] https://medium.com/@abhishekjainindore24/demystifying-ai-ml-deep-learning-and-data-science-a-beginners-guide-887e39d172db\n",
            "  5. [What is Deep Learning? - by David DeBaggis - Substack] https://substack.com/home/post/p-138158217?utm_campaign=post&utm_medium=web\n",
            "--- Results for query: '\"deep learning architecture components\"' ---\n",
            "  1. [Deep Learning - London School of Innovation] https://lsi-ac.uk/spec/module/online/6de3b85b-4725-4fe1-8587-ed1908851a40\n",
            "  2. [The Next Generation of Machine Learning Tools - Roman Ring] http://inoryy.com/post/next-gen-ml-tools/\n",
            "  3. [Deep learning architecture components. A DenseNet encoding ...] https://www.researchgate.net/figure/Deep-learning-architecture-components-A-DenseNet-encoding-section-precedes-the-decoding_fig1_379539196\n",
            "  4. [[PDF] DEEPREBIRTH:AGENERAL APPROACH FOR ACCEL- ERATING ...] https://openreview.net/references/pdf?id=SkwSJ99ex\n",
            "  5. [[PDF] erating deep neural network execution on - OpenReview] https://openreview.net/pdf?id=SkwSJ99ex\n",
            "--- Results for query: '\"types deep learning architectures applications CNN RNN GAN\"' ---\n",
            "  No results found.\n",
            "--- Results for query: '\"deep learning advancements last 10 years\"' ---\n",
            "  No results found.\n",
            "--- Results for query: '\"deep learning breakthroughs milestones\"' ---\n",
            "  No results found.\n",
            "--- Results for query: '\"computational power datasets impact deep learning\"' ---\n",
            "  No results found.\n",
            "--- Results for query: '\"deep learning limitations challenges\"' ---\n",
            "  No results found.\n",
            "--- Results for query: '\"ethical considerations deep learning\"' ---\n",
            "  1. [How Deep Learning is Revolutionizing the Way Small Businesses ...] https://medium.com/@pcsocial/how-deep-learning-is-revolutionizing-the-way-small-businesses-operate-ae9e32f8f0ff\n",
            "  2. [From Theory to Practice: Exploring the Impact of Deep Learning in ...] https://instadatahelp.com/from-theory-to-practice-exploring-the-impact-of-deep-learning-in-education/\n",
            "  3. [Ethical And Societal Implications Of Deep Learning - FasterCapital] https://fastercapital.com/topics/ethical-and-societal-implications-of-deep-learning.html\n",
            "  4. [Ethical Considerations in Deep Learning: Navigating the AI Minefield] https://www.linkedin.com/pulse/ethical-considerations-deep-learning-navigating-ai-minefield-sachin-u8p9c\n",
            "  5. [Challenges And Ethical Considerations In Conversational Ai ...] https://fastercapital.com/topics/challenges-and-ethical-considerations-in-conversational-ai.html/2\n",
            "--- Results for query: '\"future trends deep learning research\"' ---\n",
            "  No results found.\n",
            "--- Results for query: '\"deep learning applications examples image recognition NLP healthcare\"' ---\n",
            "  No results found.\n",
            "--- Results for query: '\"deep learning mathematical statistical concepts\"' ---\n",
            "  No results found.\n",
            "--- Results for query: '\"deep learning vs machine learning\"' ---\n",
            "  1. [Deep Learning vs. Machine Learning: A Beginner's Guide - Coursera] https://www.coursera.org/articles/ai-vs-deep-learning-vs-machine-learning-beginners-guide\n",
            "  2. [Deep learning vs. machine learning: A complete guide - Zendesk] https://www.zendesk.com/blog/machine-learning-and-deep-learning/\n",
            "  3. [Deep Learning VS Machine Learning - AI For Everyone] https://community.deeplearning.ai/t/deep-learning-vs-machine-learning/527721\n",
            "  4. [Deep learning vs machine learning | Google Cloud] https://cloud.google.com/discover/deep-learning-vs-machine-learning\n",
            "  5. [Deep Learning vs Machine Learning - Difference Between ... - AWS] https://aws.amazon.com/compare/the-difference-between-machine-learning-and-deep-learning/\n",
            "--- Results for query: '\"deep learning software libraries tools TensorFlow PyTorch\"' ---\n",
            "  No results found.\n",
            "--- Results for query: '\"deep learning online courses tutorials\"' ---\n",
            "  1. [What is Deep Learning? The Ultimate Beginner's Guide] https://ai-solutions.pro/what-is-deep-learning-the-ultimate-beginners-guide/\n",
            "  2. [Abdul Qadeer - artificialintelligence #learnai #aicareer - LinkedIn] https://www.linkedin.com/posts/abdulqadeerpor_artificialintelligence-learnai-aicareer-activity-7290088798955216896-1BLq\n",
            "  3. [How Deep Learning Can Boost Your Marketing Campaigns - LinkedIn] https://www.linkedin.com/advice/3/how-can-deep-learning-help-marketers-create-cr8gf\n",
            "  4. [[PDF] Deep Learning 1nbsped] https://app.pulsar.uba.ar/HomePages/scholarship/T68752/Deep_Learning_1nbsped.pdf\n",
            "  5. [CSE IN THE AGE OF AI: PREPARING FOR A FUTURE IN MACHINE ...] https://blog.puranmurti.com/cse-in-the-age-of-ai-preparing-for-a-future-in-machine-learning/\n",
            "--- Results for query: '\"key researchers deep learning\"' ---\n",
            "  No results found.\n",
            "\n",
            "Collected 25 total raw search results across all queries.\n",
            "\n",
            "---Executing Crawler---\n",
            "Found 25 unique URLs from search. Crawling up to 5...\n",
            "Crawling: https://substack.com/home/post/p-138158217?utm_campaign=post&utm_medium=web\n",
            "Successfully crawled and extracted text from https://substack.com/home/post/p-138158217?utm_campaign=post&utm_medium=web\n",
            "Crawling: https://www.linkedin.com/pulse/ethical-considerations-deep-learning-navigating-ai-minefield-sachin-u8p9c\n",
            "Successfully crawled and extracted text from https://www.linkedin.com/pulse/ethical-considerations-deep-learning-navigating-ai-minefield-sachin-u8p9c\n",
            "Crawling: https://app.pulsar.uba.ar/HomePages/scholarship/T68752/Deep_Learning_1nbsped.pdf\n",
            "Successfully crawled and extracted text from https://app.pulsar.uba.ar/HomePages/scholarship/T68752/Deep_Learning_1nbsped.pdf\n",
            "Crawling: https://www.linkedin.com/advice/3/how-can-deep-learning-help-marketers-create-cr8gf\n",
            "Successfully crawled and extracted text from https://www.linkedin.com/advice/3/how-can-deep-learning-help-marketers-create-cr8gf\n",
            "Crawling: https://www.researchgate.net/figure/Deep-learning-architecture-components-A-DenseNet-encoding-section-precedes-the-decoding_fig1_379539196\n",
            "Error fetching https://www.researchgate.net/figure/Deep-learning-architecture-components-A-DenseNet-encoding-section-precedes-the-decoding_fig1_379539196: 403 Client Error: Forbidden for url: https://www.researchgate.net/figure/Deep-learning-architecture-components-A-DenseNet-encoding-section-precedes-the-decoding_fig1_379539196\n",
            "Finished crawling. Collected content from 4 pages.\n",
            "\n",
            "---Executing Information Processor---\n",
            "Information processing complete.\n",
            "\n",
            "---Executing Drafting Agent---\n",
            "Drafting complete.\n",
            "\n",
            "---Executing Reviewer---\n",
            "Raw Reviewer Output:\n",
            "DECISION: ACCEPT - The draft is acceptable.\n",
            "\n",
            "The draft provides a good overview of deep learning, covering its definition, advantages, disadvantages, and ethical considerations.  It's well-structured, clear, and concise.  The language is accurate and appropriate for a general audience.  There are no grammatical errors.  The inclusion of specific examples of deep learning architectures (CNNs, RNNs, DenseNet, etc.) adds value.  While it could perhaps mention some specific applications of deep learning to further illustrate its capabilities, this is not strictly necessary for a general explanation.  The overall quality is high enough to be considered acceptable without significant revisions.\n",
            "Parsed Review Decision: END\n",
            "Parsed Review Feedback: The draft provides a good overview of deep learning, covering its definition, advantages, disadvantages, and ethical considerations.  It's well-structured, clear, and concise.  The language is accurate and appropriate for a general audience.  There are no grammatical errors.  The inclusion of specific examples of deep learning architectures (CNNs, RNNs, DenseNet, etc.) adds value.  While it could perhaps mention some specific applications of deep learning to further illustrate its capabilities, this is not strictly necessary for a general explanation.  The overall quality is high enough to be considered acceptable without significant revisions.\n",
            "\n",
            "---Executing Review Router (Function)---\n",
            "Reviewer decision received: END\n",
            "Review accepted or decision unclear. Ending workflow.\n",
            "\n",
            "--- Workflow Finished ---\n",
            "\n",
            "--- Final Answer Draft ---\n",
            "Deep learning is a specialized area within machine learning and artificial intelligence.  It utilizes artificial neural networks with multiple layers (deep neural networks) to analyze unstructured data, enabling it to cluster, classify, and predict outcomes.  This multi-layered approach allows deep learning algorithms to learn from errors through iterative processes, requiring less direct human intervention compared to simpler machine learning techniques.  However, this power comes at a cost:  deep learning necessitates extensive datasets, often including diverse and unstructured information.\n",
            "\n",
            "Various deep learning architectures exist, including Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), along with more advanced models like DenseNet, ResNet, GoogLeNet, and transformers.  These architectures are frequently combined in complex applications.\n",
            "\n",
            "Despite its capabilities, ethical considerations are crucial.  Potential issues include algorithmic bias leading to unfair outcomes, privacy violations, misuse of the technology, and a lack of transparency in decision-making.  Resource constraints, particularly the need for large datasets and significant computational power, pose challenges, especially for smaller businesses.  Therefore, careful consideration of these ethical and practical implications is essential before implementing deep learning systems; simpler machine learning methods may be more appropriate for less complex tasks.\n",
            "\n",
            "--- Final Review Feedback ---\n",
            "The draft provides a good overview of deep learning, covering its definition, advantages, disadvantages, and ethical considerations.  It's well-structured, clear, and concise.  The language is accurate and appropriate for a general audience.  There are no grammatical errors.  The inclusion of specific examples of deep learning architectures (CNNs, RNNs, DenseNet, etc.) adds value.  While it could perhaps mention some specific applications of deep learning to further illustrate its capabilities, this is not strictly necessary for a general explanation.  The overall quality is high enough to be considered acceptable without significant revisions.\n",
            "\n",
            "--- Final Review Decision ---\n",
            "END\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AFRcMAXrnONW"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}